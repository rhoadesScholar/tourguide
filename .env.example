# AI Narration Configuration
# Choose ONE of the following options:

# Option 1: Cloud AI (Gemini - recommended)
# Get your API key from: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=your_google_api_key_here

# Option 2: Cloud AI (Claude/Anthropic)
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Option 3: Local AI (Ollama + Kokoro TTS - no API key needed)
# Set to true to use local mode (requires Ollama and llama3.2-vision model)
# Install: Download from ollama.com, then run: ollama pull llama3.2-vision
# For TTS: pip install kokoro soundfile sounddevice
USE_LOCAL=false

# Optional: Force specific provider (gemini|claude|local|auto)
# AI_PROVIDER=auto

# Ollama Configuration
# Optional: Custom path for Ollama models (defaults to ~/.ollama/models)
# OLLAMA_MODELS=/path/to/your/ollama-models

# TTS (Text-to-Speech) Configuration
# Choose between Chatterbox (voice cloning) or edge-tts (cloud)

# Option 1: Chatterbox TTS with voice cloning (recommended for custom voices)
# Set to true to enable Chatterbox instead of edge-tts
USE_CHATTERBOX=false

# Optional: Path to audio file for voice cloning (10-30 seconds of clear speech)
# VOICE_REFERENCE_PATH=/path/to/your/voice_sample.wav

# Option 2: edge-tts (cloud-based, no voice cloning)
# Voice to use when Chatterbox is disabled
# Available voices: en-GB-RyanNeural (British male), en-US-AriaNeural (American female), etc.
EDGE_VOICE=en-GB-RyanNeural

# Organelle Database Configuration (for Query Mode)
# Path where SQLite database will be created
ORGANELLE_DB_PATH=./organelle_data/organelles.db

# CSV file paths (comma-separated)
# Format: path/to/file.csv
# Organelle type will be inferred from filename (e.g., mito.csv â†’ mitochondria)
# ORGANELLE_CSV_PATHS=./organelle_data/mito.csv,./organelle_data/nucleus.csv,./organelle_data/er.csv

# AI model for query processing (via Ollama)
# Install: ollama pull nemotron
QUERY_AI_MODEL=nemotron-3-nano

# AI model for analysis code generation
# If using Ollama (USE_LOCAL=true), specify the model:
# ANALYSIS_AI_MODEL=qwen2.5-coder:1.5b
# If using Gemini, this will use GEMINI_MODEL by default
# If using Claude, uses claude-3-5-sonnet by default
# ANALYSIS_AI_MODEL=qwen2.5-coder:7b

# Analysis Mode Configuration (for container-based code execution)
# Enable analysis mode (requires Docker or Apptainer)
ENABLE_ANALYSIS_MODE=true

# Container Runtime Configuration
# Options: auto (default), docker, apptainer, singularity
# auto = automatically detect available runtime (prefers apptainer > docker)
CONTAINER_RUNTIME=auto

# Container execution timeout in seconds
DOCKER_TIMEOUT=60

# Container memory limit (e.g., 512m, 1g)
# Note: Docker honors this directly; Apptainer may rely on LSF/scheduler
DOCKER_MEMORY_LIMIT=512m

# Directory for storing analysis results
ANALYSIS_RESULTS_DIR=./analysis_results

# Apptainer-specific Configuration
# Path to Apptainer SIF image (optional, defaults to ./containers/tourguide-analysis.sif)
# APPTAINER_IMAGE_PATH=./containers/tourguide-analysis.sif
